{
  "//": "========================================",
  "//": "LlamaCpp Backend Configuration Example",
  "//": "========================================",
  "//": "",
  "//": "This example demonstrates how to configure the LlamaCpp backend",
  "//": "with automatic model downloading from HuggingFace or other sources.",
  "//": "",
  "//": "Prerequisites:",
  "//": "1. Download and run llama-server from llama.cpp",
  "//": "   GitHub: https://github.com/ggerganov/llama.cpp",
  "//": "",
  "//": "2. Start the server (example):",
  "//": "   ./llama-server -m ./models/model.gguf -c 4096 --port 8080",
  "//": "",
  "//": "OR use the auto-download feature:",
  "//": "3. Just configure ModelPath and ModelUrl below",
  "//": "4. The backend will automatically download the model on first use",
  "//": "",
  "LlmSettings": {
    "SelectionStrategy": "Failover",

    "Backends": [
      {
        "//": "========== Example 1: Auto-Download from HuggingFace ==========",
        "Name": "LlamaCpp-Llama3-8B",
        "Type": "LlamaCpp",
        "BaseUrl": "http://localhost:8080",
        "ApiKey": null,
        "ModelName": "llama-3-8b-instruct-q4",

        "//": "----- Model Download Configuration -----",
        "ModelPath": "./models/llama-3-8b-instruct-q4_k_m.gguf",
        "ModelUrl": "https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf",
        "AutoDownloadModel": true,

        "//": "----- LlamaCpp-Specific Settings -----",
        "ContextSize": 4096,
        "GpuLayers": 0,
        "Threads": null,
        "UseMemoryLock": false,
        "Seed": -1,

        "//": "----- Standard LLM Settings -----",
        "Temperature": 0.7,
        "MaxInputTokens": 4096,
        "MaxOutputTokens": 2000,
        "TopP": 0.95,
        "FrequencyPenalty": 0.0,
        "PresencePenalty": 0.0,
        "StopSequences": ["</s>", "[INST]", "[/INST]"],

        "Priority": 1,
        "Enabled": true,
        "TimeoutSeconds": 300,
        "CostPerMillionInputTokens": 0,
        "CostPerMillionOutputTokens": 0
      },

      {
        "//": "========== Example 2: Mistral 7B with GPU Acceleration ==========",
        "Name": "LlamaCpp-Mistral-7B-GPU",
        "Type": "LlamaCpp",
        "BaseUrl": "http://localhost:8081",
        "ModelName": "mistral-7b-instruct-v0.2",

        "ModelPath": "./models/mistral-7b-instruct-v0.2-q5_k_m.gguf",
        "ModelUrl": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q5_K_M.gguf",
        "AutoDownloadModel": true,

        "//": "GPU Settings: Offload 32 layers to GPU for faster inference",
        "ContextSize": 8192,
        "GpuLayers": 32,
        "Threads": 8,
        "UseMemoryLock": true,

        "Temperature": 0.8,
        "MaxOutputTokens": 4000,
        "Priority": 2,
        "Enabled": false
      },

      {
        "//": "========== Example 3: TinyLlama 1.1B (Fast, CPU-only) ==========",
        "Name": "LlamaCpp-TinyLlama",
        "Type": "LlamaCpp",
        "BaseUrl": "http://localhost:8082",
        "ModelName": "tinyllama-1.1b-chat",

        "ModelPath": "./models/tinyllama-1.1b-chat-v1.0-q4_k_m.gguf",
        "ModelUrl": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
        "AutoDownloadModel": true,

        "//": "Lightweight model for quick responses",
        "ContextSize": 2048,
        "GpuLayers": 0,
        "Threads": 4,

        "Temperature": 0.7,
        "MaxOutputTokens": 512,
        "Priority": 3,
        "Enabled": false
      },

      {
        "//": "========== Example 4: Existing Model (No Download) ==========",
        "Name": "LlamaCpp-Existing-Model",
        "Type": "LlamaCpp",
        "BaseUrl": "http://localhost:8083",
        "ModelName": "custom-model",

        "//": "Model already exists, no download needed",
        "ModelPath": "/path/to/existing/model.gguf",
        "ModelUrl": null,
        "AutoDownloadModel": false,

        "ContextSize": 2048,
        "Priority": 4,
        "Enabled": false
      }
    ]
  },

  "//": "===========================================",
  "//": "Configuration Property Documentation",
  "//": "===========================================",
  "//": "",
  "//": "ModelPath (string):",
  "//": "  - Local filesystem path where the GGUF model file should be stored",
  "//": "  - Can be relative (./models/...) or absolute (/home/user/models/...)",
  "//": "  - Directory will be created automatically if it doesn't exist",
  "//": "",
  "//": "ModelUrl (string):",
  "//": "  - HTTP/HTTPS URL to download the model from if it doesn't exist locally",
  "//": "  - Commonly used sources:",
  "//": "    * HuggingFace: https://huggingface.co/...",
  "//": "    * Direct GGUF files from model repositories",
  "//": "  - Set to null if you don't want auto-download",
  "//": "",
  "//": "AutoDownloadModel (bool):",
  "//": "  - If true: automatically downloads model from ModelUrl if ModelPath doesn't exist",
  "//": "  - If false: expects model to already exist at ModelPath",
  "//": "  - Default: true",
  "//": "",
  "//": "ContextSize (int):",
  "//": "  - Maximum context window size in tokens",
  "//": "  - Common values: 2048, 4096, 8192, 16384, 32768",
  "//": "  - Larger = more memory but can handle longer conversations",
  "//": "  - Default: 2048",
  "//": "",
  "//": "GpuLayers (int):",
  "//": "  - Number of model layers to offload to GPU",
  "//": "  - 0 = CPU only (slower but works everywhere)",
  "//": "  - Higher = more GPU usage (faster but requires GPU with enough VRAM)",
  "//": "  - Typical values: 0 (CPU), 20-40 (hybrid), 99 (all on GPU)",
  "//": "  - Default: 0",
  "//": "",
  "//": "Threads (int):",
  "//": "  - Number of CPU threads to use for inference",
  "//": "  - null = auto-detect (recommended)",
  "//": "  - Set manually for fine-tuning performance",
  "//": "  - Default: null",
  "//": "",
  "//": "UseMemoryLock (bool):",
  "//": "  - Lock model in RAM to prevent swapping to disk",
  "//": "  - Improves performance but requires enough physical RAM",
  "//": "  - Default: false",
  "//": "",
  "//": "Seed (int):",
  "//": "  - Random seed for reproducible outputs",
  "//": "  - -1 = random seed each time",
  "//": "  - Set to specific number for deterministic outputs",
  "//": "  - Default: -1",
  "//": "",
  "//": "===========================================",
  "//": "Popular Model Sources",
  "//": "===========================================",
  "//": "",
  "//": "TheBloke (HuggingFace):",
  "//": "  https://huggingface.co/TheBloke",
  "//": "  - Huge collection of quantized GGUF models",
  "//": "  - Various quantization levels (Q4, Q5, Q8, etc.)",
  "//": "",
  "//": "QuantFactory (HuggingFace):",
  "//": "  https://huggingface.co/QuantFactory",
  "//": "  - Latest models in GGUF format",
  "//": "",
  "//": "Model Recommendations:",
  "//": "  - Small/Fast (1-3B): TinyLlama, Phi-2",
  "//": "  - Medium (7B): Llama-3-8B, Mistral-7B, Zephyr-7B",
  "//": "  - Large (13B+): Llama-2-13B, Mixtral-8x7B",
  "//": "",
  "//": "Quantization Levels (size vs quality):",
  "//": "  - Q4_K_M: Good quality, smaller size (recommended for most uses)",
  "//": "  - Q5_K_M: Better quality, medium size",
  "//": "  - Q8_0: High quality, larger size",
  "//": "  - Q2/Q3: Very small but lower quality",
  "//": "",
  "//": "===========================================",
  "//": "Logging & Monitoring",
  "//": "===========================================",
  "//": "",
  "//": "The LlamaCpp backend provides detailed logging:",
  "//": "  - Model download start/progress/completion",
  "//": "  - Download progress logged every 5 seconds",
  "//": "  - Warnings if ModelPath or ModelUrl not configured",
  "//": "  - Errors if download fails",
  "//": "  - Health check status",
  "//": "",
  "//": "Example log output:",
  "//": "  [LlamaCpp-Llama3-8B] Downloading model from https://...",
  "//": "  [LlamaCpp-Llama3-8B] Download progress: 25.3% (512MB / 2GB)",
  "//": "  [LlamaCpp-Llama3-8B] Model downloaded successfully (2.1GB)",
  "//": ""
}
