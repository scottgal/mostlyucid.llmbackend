{
  "LlmSettings": {
    "//": "========== SELECTION STRATEGY ==========",
    "SelectionStrategy": "Failover",
    "//SelectionStrategy-Options": "Failover, RoundRobin, LowestLatency, Specific, Random",

    "//": "========== GLOBAL DEFAULTS ==========",
    "TimeoutSeconds": 120,
    "MaxRetries": 3,
    "UseExponentialBackoff": true,
    "RetryDelayMs": 1000,
    "DefaultTemperature": 0.7,
    "DefaultMaxTokens": 2000,

    "//": "========== CIRCUIT BREAKER ==========",
    "CircuitBreaker": {
      "Enabled": true,
      "FailureThreshold": 5,
      "DurationOfBreakSeconds": 30,
      "SamplingDurationSeconds": 60,
      "MinimumThroughput": 10
    },

    "//": "========== RATE LIMITING ==========",
    "RateLimit": {
      "Enabled": false,
      "MaxRequests": 100,
      "WindowSeconds": 60,
      "MaxConcurrentRequests": 10,
      "QueueLimit": 100
    },

    "//": "========== CACHING ==========",
    "Caching": {
      "Enabled": false,
      "//Provider-Options": "Memory, Redis, SqlServer, NCache, Custom",
      "Provider": "Memory",
      "ExpirationMinutes": 60,
      "MaxCacheSizeMb": 100,
      "ConnectionString": null,
      "KeyPrefix": "llm:"
    },

    "//": "========== HEALTH CHECKS ==========",
    "HealthCheck": {
      "Enabled": true,
      "IntervalSeconds": 60,
      "TimeoutSeconds": 10,
      "UnhealthyThreshold": 3,
      "HealthyThreshold": 2
    },

    "//": "========== SECRETS MANAGEMENT ==========",
    "Secrets": {
      "//Provider-Options": "Configuration, EnvironmentVariables, AzureKeyVault, AwsSecretsManager, HashiCorpVault, GoogleSecretManager, Custom",
      "Provider": "Configuration",
      "KeyVaultUrl": null,
      "AwsRegion": null,
      "EnvironmentVariablePrefix": "LLM_",
      "UseManagedIdentity": true
    },

    "//": "========== TELEMETRY ==========",
    "Telemetry": {
      "EnableMetrics": true,
      "EnableTracing": true,
      "EnableDetailedLogging": false,
      "LogContent": false,
      "ServiceName": "LlmBackend",
      "EnableCostTracking": false
    },

    "//": "========== CONTEXT MEMORY ==========",
    "Memory": {
      "//Provider-Options": "InMemory, Redis, SqlServer, CosmosDb, File, Custom",
      "Provider": "InMemory",
      "ConnectionString": null,
      "DefaultTokenLimit": 4096,
      "EnableCompression": false,
      "TtlMinutes": 60
    },

    "//": "========== BACKENDS ==========",
    "Backends": [
      {
        "//": "---------- OpenAI GPT-4 ----------",
        "Name": "OpenAI-GPT4",
        "Type": "OpenAI",
        "BaseUrl": "https://api.openai.com",
        "ApiKey": "sk-YOUR_OPENAI_API_KEY",
        "ModelName": "gpt-4o",
        "OrganizationId": null,
        "Temperature": null,
        "MaxInputTokens": 128000,
        "MaxOutputTokens": 4096,
        "TopP": null,
        "FrequencyPenalty": null,
        "PresencePenalty": null,
        "StopSequences": null,
        "Priority": 1,
        "Enabled": true,
        "TimeoutSeconds": null,
        "MaxRetries": null,
        "EnableStreaming": true,
        "EnableFunctionCalling": true,
        "CostPerMillionInputTokens": 5.0,
        "CostPerMillionOutputTokens": 15.0,
        "AdditionalHeaders": null
      },
      {
        "//": "---------- Anthropic Claude 3.5 ----------",
        "Name": "Anthropic-Claude",
        "Type": "Anthropic",
        "BaseUrl": "https://api.anthropic.com",
        "ApiKey": "sk-ant-YOUR_ANTHROPIC_API_KEY",
        "ModelName": "claude-3-5-sonnet-20241022",
        "AnthropicVersion": "2023-06-01",
        "Temperature": null,
        "MaxInputTokens": 200000,
        "MaxOutputTokens": 4096,
        "TopP": null,
        "StopSequences": null,
        "Priority": 2,
        "Enabled": true,
        "EnableStreaming": false,
        "EnableFunctionCalling": false,
        "CostPerMillionInputTokens": 3.0,
        "CostPerMillionOutputTokens": 15.0
      },
      {
        "//": "---------- Azure OpenAI ----------",
        "Name": "Azure-GPT4",
        "Type": "AzureOpenAI",
        "BaseUrl": "https://YOUR_RESOURCE_NAME.openai.azure.com",
        "ApiKey": "YOUR_AZURE_OPENAI_KEY",
        "DeploymentName": "gpt-4",
        "ApiVersion": "2024-02-15-preview",
        "ModelName": null,
        "Temperature": null,
        "MaxInputTokens": 128000,
        "MaxOutputTokens": 4096,
        "Priority": 3,
        "Enabled": false
      },
      {
        "//": "---------- Google Gemini (AI Studio) ----------",
        "Name": "Gemini-Pro",
        "Type": "Gemini",
        "BaseUrl": "https://generativelanguage.googleapis.com",
        "ApiKey": "YOUR_GOOGLE_API_KEY",
        "ModelName": "gemini-1.5-pro",
        "Temperature": null,
        "MaxInputTokens": 1000000,
        "MaxOutputTokens": 8192,
        "Priority": 4,
        "Enabled": false,
        "CostPerMillionInputTokens": 1.25,
        "CostPerMillionOutputTokens": 5.0
      },
      {
        "//": "---------- Google Gemini (Vertex AI) ----------",
        "Name": "Gemini-Vertex",
        "Type": "Gemini",
        "BaseUrl": "https://us-central1-aiplatform.googleapis.com",
        "ProjectId": "YOUR_GCP_PROJECT_ID",
        "Location": "us-central1",
        "ModelName": "gemini-1.5-pro",
        "Priority": 5,
        "Enabled": false
      },
      {
        "//": "---------- Cohere ----------",
        "Name": "Cohere-Command",
        "Type": "Cohere",
        "BaseUrl": "https://api.cohere.ai",
        "ApiKey": "YOUR_COHERE_API_KEY",
        "ModelName": "command-r-plus",
        "Temperature": null,
        "MaxOutputTokens": 4000,
        "Priority": 6,
        "Enabled": false,
        "CostPerMillionInputTokens": 3.0,
        "CostPerMillionOutputTokens": 15.0
      },
      {
        "//": "---------- Ollama (Local) ----------",
        "Name": "Local-Ollama",
        "Type": "Ollama",
        "BaseUrl": "http://localhost:11434",
        "ModelName": "llama3",
        "Temperature": null,
        "MaxOutputTokens": 2000,
        "Priority": 90,
        "Enabled": false,
        "CostPerMillionInputTokens": 0,
        "CostPerMillionOutputTokens": 0
      },
      {
        "//": "---------- LM Studio (Local) ----------",
        "Name": "Local-LMStudio",
        "Type": "LMStudio",
        "BaseUrl": "http://localhost:1234",
        "ModelName": "local-model",
        "Priority": 91,
        "Enabled": false
      },
      {
        "//": "---------- EasyNMT (Translation) ----------",
        "Name": "EasyNMT-Translation",
        "Type": "EasyNMT",
        "BaseUrl": "http://localhost:24080",
        "ModelName": "opus-mt",
        "Priority": 95,
        "Enabled": false
      },
      {
        "//": "---------- Generic OpenAI-Compatible ----------",
        "Name": "Generic-OpenAI",
        "Type": "GenericOpenAI",
        "BaseUrl": "https://your-custom-endpoint.com",
        "ApiKey": "YOUR_API_KEY",
        "ModelName": "your-model",
        "Priority": 99,
        "Enabled": false
      },
      {
        "//": "---------- LlamaCpp (Local with Auto-Download) ----------",
        "Name": "LlamaCpp-Local",
        "Type": "LlamaCpp",
        "BaseUrl": "http://localhost:8080",
        "ApiKey": null,
        "ModelName": "llama-3-8b-instruct",
        "//": "Model file configuration",
        "ModelPath": "./models/llama-3-8b-instruct-q4_k_m.gguf",
        "ModelUrl": "https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf",
        "AutoDownloadModel": true,
        "//": "LlamaCpp-specific settings",
        "ContextSize": 4096,
        "GpuLayers": 0,
        "Threads": null,
        "UseMemoryLock": false,
        "Seed": -1,
        "//": "Standard LLM settings",
        "Temperature": 0.7,
        "MaxOutputTokens": 2000,
        "TopP": 0.95,
        "FrequencyPenalty": 0.0,
        "PresencePenalty": 0.0,
        "Priority": 92,
        "Enabled": false,
        "CostPerMillionInputTokens": 0,
        "CostPerMillionOutputTokens": 0,
        "//": "Notes",
        "//Note1": "ModelPath: Local path where the GGUF model file should be stored",
        "//Note2": "ModelUrl: URL to download the model from if it doesn't exist (HuggingFace, etc.)",
        "//Note3": "AutoDownloadModel: If true, model will be downloaded automatically on first use",
        "//Note4": "GpuLayers: Number of layers to offload to GPU (0 = CPU only, higher = more GPU)",
        "//Note5": "ContextSize: Context window size in tokens (default: 2048)",
        "//Note6": "Threads: Number of CPU threads to use (null = auto-detect)",
        "//Note7": "Requires llama.cpp server running on BaseUrl (use llama-server executable)"
      }
    ]
  }
}
