<Project Sdk="Microsoft.NET.Sdk">

    <PropertyGroup>
        <TargetFramework>net8.0</TargetFramework>
        <ImplicitUsings>enable</ImplicitUsings>
        <Nullable>enable</Nullable>
        <GeneratePackageOnBuild>true</GeneratePackageOnBuild>
        <GenerateDocumentationFile>true</GenerateDocumentationFile>

        <!-- Package Metadata -->
        <PackageId>Mostlyucid.LlmBackend.Providers.LlamaCpp</PackageId>
        <Version>3.0.0</Version>
        <Authors>Scott Galloway</Authors>
        <Company>Mostlyucid</Company>
        <Product>Mostlyucid LLM Backend - LlamaCpp Provider</Product>
        <Description>LlamaCpp provider for Mostlyucid.LlmBackend. Supports locally-hosted GGUF models with automatic downloading, ideal for running tiny embedded models. Requires the core Mostlyucid.LlmBackend package.</Description>
        <PackageTags>LLM;LlamaCpp;Local;GGUF;Embedded;TinyModels;Llama;AI;Provider</PackageTags>
        <PackageReadmeFile>README.md</PackageReadmeFile>
        <PackageLicenseExpression>MIT</PackageLicenseExpression>
        <PackageProjectUrl>https://github.com/scottgal/mostlyucid.llmbackend</PackageProjectUrl>
        <RepositoryUrl>https://github.com/scottgal/mostlyucid.llmbackend</RepositoryUrl>
        <RepositoryType>git</RepositoryType>
        <PublishRepositoryUrl>true</PublishRepositoryUrl>
        <IncludeSymbols>true</IncludeSymbols>
        <SymbolPackageFormat>snupkg</SymbolPackageFormat>

        <!-- Release Notes -->
        <PackageReleaseNotes>
v3.0.0 - Initial release as separate provider package
- LlamaCpp local model support with GGUF format
- Automatic model downloading from URLs
- OpenAI-compatible API support
- Ideal for tiny embedded models
- CPU and GPU offloading support
        </PackageReleaseNotes>
    </PropertyGroup>

    <ItemGroup>
        <None Include="README.md" Pack="true" PackagePath="\" />
    </ItemGroup>

    <ItemGroup>
        <ProjectReference Include="..\Mostlyucid.LlmBackend\Mostlyucid.LlmBackend.csproj" />
    </ItemGroup>

</Project>
